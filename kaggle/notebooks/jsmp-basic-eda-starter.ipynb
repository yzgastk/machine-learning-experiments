{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow import random\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sn\n\n\nrandom.set_seed(5577)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrainDf = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reducing Memory Usage\nI had trouble to use the dataset due to it using around 5GB of RAM just after being loaded. I found this function from [sbunzini](https://www.kaggle.com/sbunzini/reduce-memory-usage-by-75) to mitigate the issue."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):\n    start_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n\n    end_memory = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n    return df\n\ntrainDf = reduce_memory_usage(trainDf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Removing columns that will not be used as feature for the training phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"dropCols = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"ts_id\"]\ntrainDf = trainDf.drop(columns=dropCols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling \"na\" with 0 for starter. It might be wiser to use some other techniques (imputing, mean, ...) but for a first version, this will do the job."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data tab of the competition :\n> Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n\nSo, I make a slice without those rows before looking into the details."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDfW = trainDf[trainDf[\"weight\"] > 0]\ntrainDfW.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\n### Basics"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDfW.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDfW.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDfW.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Understanding"},{"metadata":{},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncorrDfW = trainDfW.corr()\nfig, ax = plt.subplots(figsize=(25,25)) \nsn.heatmap(corrDfW, linewidths=.5, annot=False, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the matrix is quite heavy, it allows to identify some interesting clusters. Some features seems highly (positively or negatively) correlated. The next step would be to pinpoint those features and check from features.csv if they share the same tags... ToDo"},{"metadata":{},"cell_type":"markdown","source":"### PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nscaler = MinMaxScaler()\nscaledTrain = scaler.fit_transform(trainDfW)\n\npca = PCA().fit(scaledTrain)\nexCumul = np.cumsum(pca.explained_variance_ratio_)\npx.area(\n    x=range(1, exCumul.shape[0] + 1),\n    y=exCumul,\n    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that :\n* One of the component accounts for a third (36.9%) of the total variance\n* The threshold of 90% variance is explained by 8 components\n* The threshold of 95% variance is explained by 11 components"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\ndfComp = pca.fit_transform(scaledTrain)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(dfComp, x=0, y=1, color=trainDfW['weight'], title=f'Total Explained Variance: {total_var:.3f}%', labels={'0': 'PC 1', '1': 'PC 2'})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets take a look of the two major principal components when we remove feature_0 from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfNoF0 = trainDfW.drop(\"feature_0\", 1)\nscaledTrainNoF0 = scaler.fit_transform(dfNoF0)\npca = PCA(n_components=2)\ndfComp = pca.fit_transform(scaledTrainNoF0)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(dfComp, x=0, y=1, color=trainDfW['weight'], title=f'Total Explained Variance: {total_var:.3f}%', labels={'0': 'PC 1', '1': 'PC 2'})\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}